{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Before-your-start:\" data-toc-modified-id=\"Before-your-start:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Before your start:</a></span></li><li><span><a href=\"#Challenge-1---Import-and-Describe-the-Dataset\" data-toc-modified-id=\"Challenge-1---Import-and-Describe-the-Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Challenge 1 - Import and Describe the Dataset</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Explore-the-dataset-with-mathematical-and-visualization-techniques.-What-do-you-find?\" data-toc-modified-id=\"Explore-the-dataset-with-mathematical-and-visualization-techniques.-What-do-you-find?-2.0.0.1\"><span class=\"toc-item-num\">2.0.0.1&nbsp;&nbsp;</span>Explore the dataset with mathematical and visualization techniques. What do you find?</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-2---Data-Cleaning-and-Transformation\" data-toc-modified-id=\"Challenge-2---Data-Cleaning-and-Transformation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Challenge 2 - Data Cleaning and Transformation</a></span></li><li><span><a href=\"#Challenge-3---Data-Preprocessing\" data-toc-modified-id=\"Challenge-3---Data-Preprocessing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Challenge 3 - Data Preprocessing</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#We-will-use-the-StandardScaler-from-sklearn.preprocessing-and-scale-our-data.-Read-more-about-StandardScaler-here.\" data-toc-modified-id=\"We-will-use-the-StandardScaler-from-sklearn.preprocessing-and-scale-our-data.-Read-more-about-StandardScaler-here.-4.0.0.1\"><span class=\"toc-item-num\">4.0.0.1&nbsp;&nbsp;</span>We will use the <code>StandardScaler</code> from <code>sklearn.preprocessing</code> and scale our data. Read more about <code>StandardScaler</code> <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\" target=\"_blank\">here</a>.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-4---Data-Clustering-with-K-Means\" data-toc-modified-id=\"Challenge-4---Data-Clustering-with-K-Means-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Challenge 4 - Data Clustering with K-Means</a></span></li><li><span><a href=\"#Challenge-5---Data-Clustering-with-DBSCAN\" data-toc-modified-id=\"Challenge-5---Data-Clustering-with-DBSCAN-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Challenge 5 - Data Clustering with DBSCAN</a></span></li><li><span><a href=\"#Challenge-6---Compare-K-Means-with-DBSCAN\" data-toc-modified-id=\"Challenge-6---Compare-K-Means-with-DBSCAN-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Challenge 6 - Compare K-Means with DBSCAN</a></span></li><li><span><a href=\"#Bonus-Challenge-2---Changing-K-Means-Number-of-Clusters\" data-toc-modified-id=\"Bonus-Challenge-2---Changing-K-Means-Number-of-Clusters-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Bonus Challenge 2 - Changing K-Means Number of Clusters</a></span></li><li><span><a href=\"#Bonus-Challenge-3---Changing-DBSCAN-eps-and-min_samples\" data-toc-modified-id=\"Bonus-Challenge-3---Changing-DBSCAN-eps-and-min_samples-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Bonus Challenge 3 - Changing DBSCAN <code>eps</code> and <code>min_samples</code></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you are working with a dataset containing information about customer spending in a wholesale retail setting. The goal is to apply clustering algorithms such as K-Means and DBSCAN to segment the customers based on their purchasing behavior. By doing so, you aim to identify groups of customers with similar characteristics, which can be valuable for targeted marketing, customer segmentation, and understanding the customer base.\n",
    "\n",
    "The specific objectives of this lab may include:\n",
    "\n",
    "Exploring customer spending patterns and behavior.\n",
    "Uncovering distinct customer segments within the dataset.\n",
    "Applying clustering algorithms to identify groups of customers with similar purchasing characteristics.\n",
    "Comparing the performance of different clustering algorithms (e.g., K-Means and DBSCAN).\n",
    "Assessing the effectiveness of the clusterings and their potential insights for the business.\n",
    "Ultimately, the lab provides an opportunity to gain practical experience with clustering techniques, understand customer segmentation, and draw actionable insights from the data to drive business decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries:\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings                                              \n",
    "from sklearn.exceptions import DataConversionWarning          \n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1 - Import and Describe the Dataset\n",
    "\n",
    "In this lab, we will use a dataset containing information about customer preferences. We will look at how much each customer spends in a year on each subcategory in the grocery store and try to find similarities using clustering.\n",
    "\n",
    "The origin of the dataset is [here](https://archive.ics.uci.edu/ml/datasets/wholesale+customers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas DataFrame\n",
    "file_path = 'C:/Users/Latif-Calder√≥n/lab-unsupervised-learning-en/data/Wholesale_customers_data.csv'\n",
    "wholesale_data = pd.read_csv(file_path) #pd. is pandas module and need to be imported to run\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "print(wholesale_data.head())\n",
    "\n",
    "# Summary information about the dataset, including data types and missing values\n",
    "print(wholesale_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the dataset with mathematical and visualization techniques. What do you find?\n",
    "\n",
    "Checklist:\n",
    "\n",
    "* What does each column mean?\n",
    "* Any categorical data to convert?\n",
    "* Any missing data to remove?\n",
    "* Column collinearity - any high correlations?\n",
    "* Descriptive statistics - any outliers to remove?\n",
    "* Column-wise data distribution - is the distribution skewed?\n",
    "* Etc.\n",
    "\n",
    "Additional info: Over a century ago, an Italian economist named Vilfredo Pareto discovered that roughly 20% of the customers account for 80% of the typical retail sales. This is called the [Pareto principle](https://en.wikipedia.org/wiki/Pareto_principle). Check if this dataset displays this characteristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "#1 What does each column mean?\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "print(wholesale_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Any categorical data to convert?\n",
    "# Display the data types of the columns\n",
    "print(wholesale_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Any missing data to remove?\n",
    "# Check for missing values\n",
    "missing_values = wholesale_data.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Column collinearity - any high correlations?\n",
    "# Check for missing values\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = wholesale_data.corr()\n",
    "\n",
    "# Show the correlation matrix\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 Descriptive statistics - any outliers to remove?\n",
    "# Descriptive statistics\n",
    "print(wholesale_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 Column-wise data distribution - is the distribution skewed?\n",
    "# Data distribution for each column\n",
    "import matplotlib.pyplot as plt\n",
    "wholesale_data.hist(bins=20, figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have generated a histogram\n",
    "plt.hist(wholesale_data, bins=20)\n",
    "plt.title('Histogram of Your Data')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Save the histogram as an image file\n",
    "plt.savefig('histogram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pareto Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyze the annual spending of customers across different product categories\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Box plot of annual spending in different product categories\n",
    "plt.figure(figsize=(12, 8))\n",
    "wholesale_data.boxplot(column=['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen'])\n",
    "plt.title('Annual Spending in Different Product Categories')\n",
    "plt.ylabel('Annual Spending')\n",
    "plt.xlabel('Product Categories')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the cumulative percentage of customer spending.\n",
    "# Calculate the cumulative percentage of customer spending\n",
    "total_spending = wholesale_data.sum(axis=1)\n",
    "cumulative_percentage = total_spending.sort_values().cumsum() / total_spending.sum() * 100\n",
    "\n",
    "# Display the cumulative percentage\n",
    "print(cumulative_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate whether around 20% of the customers account for approximately 80% of the total spending\n",
    "# Evaluate the concentration of spending\n",
    "percentage_spending_80 = cumulative_percentage[cumulative_percentage <= 80].index.max() / cumulative_percentage.index.max() * 100\n",
    "percentage_customers_20 = cumulative_percentage[cumulative_percentage <= 20].index.max() / cumulative_percentage.index.max() * 100\n",
    "\n",
    "print(f\"Around {percentage_customers_20:.2f}% of the customers account for approximately {percentage_spending_80:.2f}% of the total spending\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your observations here**\n",
    "\n",
    "- ex.: Frozen, Grocery, Milk and Detergents Paper have a high...\n",
    "- ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Data Cleaning and Transformation\n",
    "\n",
    "If your conclusion from the previous challenge is the data need cleaning/transformation, do it in the cells below. However, if your conclusion is the data need not be cleaned or transformed, feel free to skip this challenge. But if you do choose the latter, please provide rationale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comment here**\n",
    "\n",
    "-  Around 100.00% of the customers account for approximately 100.00% of the total spending\n",
    "-  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Data Preprocessing\n",
    "\n",
    "One problem with the dataset is the value ranges are remarkably different across various categories (e.g. `Fresh` and `Grocery` compared to `Detergents_Paper` and `Delicassen`). If you made this observation in the first challenge, you've done a great job! This means you not only completed the bonus questions in the previous Supervised Learning lab but also researched deep into [*feature scaling*](https://en.wikipedia.org/wiki/Feature_scaling). Keep on the good work!\n",
    "\n",
    "Diverse value ranges in different features could cause issues in our clustering. The way to reduce the problem is through feature scaling. We'll use this technique again with this dataset.\n",
    "\n",
    "#### We will use the `StandardScaler` from `sklearn.preprocessing` and scale our data. Read more about `StandardScaler` [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler).\n",
    "\n",
    "*After scaling your data, assign the transformed data to a new variable `customers_scale`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your import here:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Your code here:\n",
    "# Scale the data using StandardScaler\n",
    "scaler = StandardScaler()  # Initialize the StandardScaler\n",
    "customers_scale = scaler.fit_transform(wholesale_data)  # Fit and transform the data\n",
    "\n",
    "# The scaled data is now stored in the customers_scale variable\n",
    "print(customers_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4 - Data Clustering with K-Means\n",
    "\n",
    "Now let's cluster the data with K-Means first. Initiate the K-Means model, then fit your scaled data. In the data returned from the `.fit` method, there is an attribute called `labels_` which is the cluster number assigned to each data record. What you can do is to assign these labels back to `customers` in a new column called `customers['labels']`. Then you'll see the cluster results of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize the KMeans model and specify the number of clusters\n",
    "kmeans = KMeans(n_clusters=5)  # For example, let's use 5 clusters\n",
    "\n",
    "# Fit the scaled data to the KMeans model\n",
    "kmeans.fit(customers_scale)\n",
    "\n",
    "# Assign the cluster labels to the original customers dataset\n",
    "wholesale_data['labels'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking to the elbow we can choose 2 like the correct number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_2 = KMeans(n_clusters=2).fit(customers_scale)\n",
    "\n",
    "labels = kmeans_2.predict(customers_scale)\n",
    "\n",
    "clusters = kmeans_2.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "wholesale_data['Label'] = clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the values in `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# Count the values in the 'labels' column\n",
    "label_counts = wholesale_data['Label'].value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 5 - Data Clustering with DBSCAN\n",
    "\n",
    "Now let's cluster the data using DBSCAN. Use `DBSCAN(eps=0.5)` to initiate the model, then fit your scaled data. In the data returned from the `.fit` method, assign the `labels_` back to `customers['labels_DBSCAN']`. Now your original data have two labels, one from K-Means and the other from DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN \n",
    "\n",
    "# Your code here\n",
    "# Initialize the DBSCAN model\n",
    "dbscan = DBSCAN(eps=0.5)  # Using eps=0.5 as an example\n",
    "\n",
    "# Fit the scaled data to the DBSCAN model\n",
    "dbscan.fit(customers_scale)\n",
    "\n",
    "# Assign the cluster labels to the original customers dataset\n",
    "wholesale_data['labels_DBSCAN'] = dbscan.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the values in `labels_DBSCAN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Count the values in the 'labels_DBSCAN' column\n",
    "label_counts_DBSCAN = wholesale_data['labels_DBSCAN'].value_counts()\n",
    "print(label_counts_DBSCAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 6 - Compare K-Means with DBSCAN\n",
    "\n",
    "Now we want to visually compare how K-Means and DBSCAN have clustered our data. We will create scatter plots for several columns. For each of the following column pairs, plot a scatter plot using `labels` and another using `labels_DBSCAN`. Put them side by side to compare. Which clustering algorithm makes better sense?\n",
    "\n",
    "Columns to visualize:\n",
    "\n",
    "* `Detergents_Paper` as X and `Milk` as y\n",
    "* `Grocery` as X and `Fresh` as y\n",
    "* `Frozen` as X and `Delicassen` as y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Detergents_Paper` as X and `Milk` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x,y,hue):\n",
    "    sns.scatterplot(x=x, \n",
    "                    y=y,\n",
    "                    hue=hue)\n",
    "    plt.title('Detergents Paper vs Milk ')\n",
    "    return plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "##import seaborn as sns\n",
    "##import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(x, y, hue, data, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.scatterplot(x=x, \n",
    "                    y=y,\n",
    "                    hue=hue,\n",
    "                    data=data)\n",
    "    plt.title(f'{title} - K-Means')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.scatterplot(x=x, \n",
    "                    y=y,\n",
    "                    hue='labels_DBSCAN',\n",
    "                    data=data)\n",
    "    plt.title(f'{title} - DBSCAN')\n",
    "    plt.show()\n",
    "\n",
    "# Use the 'plot' function to visualize the Detergents_Paper vs. Milk by K-Means and DBSCAN labels\n",
    "plot('Detergents_Paper', 'Milk', 'labels', wholesale_data, 'Detergents Paper vs Milk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Grocery` as X and `Fresh` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# Use the 'plot' function to visualize the Grocery vs. Fresh by K-Means and DBSCAN labels\n",
    "plot('Grocery', 'Fresh', 'labels', wholesale_data, 'Grocery vs Fresh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Frozen` as X and `Delicassen` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# Use the 'plot' function to visualize the Frozen vs. Delicassen by K-Means and DBSCAN labels\n",
    "plot('Frozen', 'Delicassen', 'labels', wholesale_data, 'Frozen vs Delicassen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a groupby to see how the mean differs between the groups. Group `customers` by `labels` and `labels_DBSCAN` respectively and compute the means for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# Group customers by K-Means labels and compute the column means\n",
    "kmeans_means = wholesale_data.groupby('labels').mean()\n",
    "print(\"K-Means Cluster Means:\")\n",
    "print(kmeans_means)\n",
    "\n",
    "# Group customers by DBSCAN labels and compute the column means\n",
    "dbscan_means = wholesale_data.groupby('labels_DBSCAN').mean()\n",
    "print(\"\\nDBSCAN Cluster Means:\")\n",
    "print(dbscan_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which algorithm appears to perform better?\n",
    "\n",
    "-The best algorithm seems to be K-Means because it seems to indentify the most recurrent customers of the products which they buy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your observations here**\n",
    "\n",
    "- When comparing K-Means and DBSCAN, there are several considerations to take into account:\n",
    "\n",
    "K-Means is sensitive to the number of clusters specified and may not perform well with irregular-shaped clusters or varying densities.\n",
    "DBSCAN is robust to noise and can find arbitrarily-shaped clusters, but it requires more care in setting its hyperparameters such as epsilon and min_samples.\n",
    "Based on the scatter plots, cluster means, and the average spending behaviors of the different clusters, you can make a comparative assessment of which algorithm better captures the inherent structure of the Wholesale customers dataset. It's important to note that the \"better\" algorithm depends on the specific characteristics of the data and the problem being addressed.\n",
    "\n",
    "As we have conducted a comparative analysis, considering the insights gained from the visualizations, the consistency and separation of the clusters, and the average spending behaviors, you can draw conclusions about which algorithm appears to better capture the structure of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 2 - Changing K-Means Number of Clusters\n",
    "\n",
    "As we mentioned earlier, we don't need to worry about the number of clusters with DBSCAN because it automatically decides that based on the parameters we send to it. But with K-Means, we have to supply the `n_clusters` param (if you don't supply `n_clusters`, the algorithm will use `8` by default). You need to know that the optimal number of clusters differs case by case based on the dataset. K-Means can perform badly if the wrong number of clusters is used.\n",
    "\n",
    "In advanced machine learning, data scientists try different numbers of clusters and evaluate the results with statistical measures (read [here](https://en.wikipedia.org/wiki/Cluster_analysis#External_evaluation)). We are not using statistical measures today but we'll use our eyes instead. In the cells below, experiment with different number of clusters and visualize with scatter plots. What number of clusters seems to work best for K-Means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:.\n",
    "# Experiment with different numbers of clusters and visualize the results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "for n_clusters, ax in zip([2, 3, 4, 5], axes.flatten()):\n",
    "    kmeans_n = KMeans(n_clusters=n_clusters)\n",
    "    kmeans_n.fit(customers_scale)\n",
    "    labels_n = kmeans_n.labels_\n",
    "    \n",
    "    sns.scatterplot(x='Detergents_Paper', y='Milk', hue=labels_n, data=wholesale_data, ax=ax)\n",
    "    ax.set_title(f'Number of clusters: {n_clusters}')\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comment here**\n",
    "\n",
    "- when the cluster is less dense , 2 clusters for example, it's easier to identify the most recurrent customers that buy detergent paper. whe the cluster is divided in more groups, each groups can be targeted to there especific needs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 3 - Changing DBSCAN `eps` and `min_samples`\n",
    "\n",
    "Experiment changing the `eps` and `min_samples` params for DBSCAN. See how the results differ with scatter plot visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "eps_values = [0.2, 0.5, 1.0]  # Example values for eps\n",
    "min_samples_values = [5, 10, 20]  # Example values for min_samples\n",
    "#this changes in comparison of the tables./fig,axes...\n",
    "fig, axes = plt.subplots(len(eps_values), len(min_samples_values), figsize=(15, 12))\n",
    "\n",
    "for i, eps in enumerate(eps_values):\n",
    "    for j, min_samples in enumerate(min_samples_values):\n",
    "        dbscan_param = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        dbscan_param.fit(customers_scale)\n",
    "        labels_param = dbscan_param.labels_\n",
    "        #this is replace with the plt.figure...\n",
    "        sns.scatterplot(x='Detergents_Paper', y='Milk', hue=labels_param, data=wholesale_data, ax=axes[i, j])\n",
    "        axes[i, j].set_title(f'eps={eps}, min_samples={min_samples}')\n",
    "        axes[i, j].legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example values for eps and min_samples\n",
    "eps_values = [0.2, 0.5, 1.0]\n",
    "min_samples_values = [5, 10, 20]\n",
    "\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan_param = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        dbscan_param.fit(customers_scale)\n",
    "        labels_param = dbscan_param.labels_\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.scatterplot(x='Detergents_Paper', y='Milk', hue=labels_param, data=wholesale_data)\n",
    "        plt.title(f'eps={eps}, min_samples={min_samples}')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comment here**\n",
    "\n",
    "- After observing the visual representations, you can analyze the effects of parameter variations on the cluster formation and determine the settings that yield the most meaningful clustering results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
